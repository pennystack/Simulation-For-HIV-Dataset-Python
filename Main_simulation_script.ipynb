{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd5edde-556b-4fad-bba3-208e8a8fe880",
   "metadata": {},
   "source": [
    "# Python HIV Dataset Simulation\n",
    "\n",
    "This notebook attempts to recreate my original R/C++ implementation for simulating an HIV patient dataset. The original code in R/C++ can be found [here](https://github.com/pennystack/Simulation-For-HIV-Dataset).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f4dca-4627-468c-bf83-003afbfd9e25",
   "metadata": {},
   "source": [
    "#### Import libraries\n",
    "\n",
    "Import all the necessary libraries to start the data simulation and estimation. The libraries **`hiv_smm`** and **`load_functions`** are custom libraries that I developed. \n",
    "\n",
    "- The **`hiv_smm`** implements optimized versions of core functions in C++, which are used for the patient dataset simulation for faster computations. \n",
    "- The **`load_functions`** handles the optimization of the likelihood function, using **JAX** for efficient gradient calculations and improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4b7d9-5405-4034-8e05-429c46f12a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "from tkinter import *\n",
    "from tkinter import simpledialog\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import scipy.stats as st\n",
    "from scipy.stats import weibull_min\n",
    "\n",
    "# My library loglikelihood C++\n",
    "import hiv_smm \n",
    "\n",
    "# My library for the optimization\n",
    "import load_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf190a-3eb9-4f30-9601-1cce98d3875f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Load the .RData files \n",
    "\n",
    "Load the parameter estimations from the real dataset and prepare the $v_{ij}$, $s_{ij}$, $a_{ij}$, $b_{ij}$ matrices for use in the data simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f161816-e7ba-418f-ac34-56eddb424da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Tk() # Tk is now available directly\n",
    "root.withdraw()\n",
    "\n",
    "# Where are the .RData files stored?\n",
    "folder_path = simpledialog.askstring(\n",
    "    \"User Input\", \n",
    "    \"Please enter the path where you have stored the .RData files: \", \n",
    "    parent = root\n",
    ")\n",
    "\n",
    "aij_path = os.path.join(folder_path, 'aij.RData')\n",
    "bij_path = os.path.join(folder_path, 'bij.RData')\n",
    "vij_path = os.path.join(folder_path, 'vij.RData')\n",
    "sij_path = os.path.join(folder_path, 'sij.RData')\n",
    "\n",
    "# Load them with pyreadr\n",
    "aij_rdata = pyreadr.read_r(aij_path)\n",
    "bij_rdata = pyreadr.read_r(bij_path)\n",
    "vij_rdata = pyreadr.read_r(vij_path)\n",
    "sij_rdata = pyreadr.read_r(sij_path)\n",
    "\n",
    "# Define the number of states - here we are examining the 4 state model\n",
    "nstates = 4\n",
    "\n",
    "# Convert them to a one-dimensional vector\n",
    "aij = aij_rdata['aij'].to_numpy().T.flatten()\n",
    "bij = bij_rdata['bij'].to_numpy().T.flatten()\n",
    "vij = vij_rdata['vij'].to_numpy()\n",
    "sij = sij_rdata['sij'].to_numpy()\n",
    "\n",
    "# Scaling parameter to control the initial parameters for the estimation - always set to 1\n",
    "parscale = 1\n",
    "\n",
    "root.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814cc80c-5ebd-4030-9470-427eb13dbb33",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Number of bootstrap simulations\n",
    "\n",
    "Set the number of bootstrap data simulations and estimations to produce.  \n",
    "\n",
    "- For statistically valid results, use 500+ samples.  \n",
    "- For quick tests, you can use 10â€“20 samples (much faster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3d7cd-21c5-4a44-a04d-1603e36d6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Tk()\n",
    "root.withdraw()\n",
    "\n",
    "# How many samples of simulated data do you want to produce?\n",
    "n_bootstrap_str = simpledialog.askstring(\n",
    "    \"User Input\", \n",
    "    \"Please enter the number of simulated data \\n (it is advised to have a minimum number of 500 simulations):\", \n",
    "    parent = root\n",
    ")\n",
    "\n",
    "n_bootstrap = int(n_bootstrap_str)\n",
    "\n",
    "# Check for potential invalid bootstrapping number\n",
    "if not isinstance(n_bootstrap, (int, float)) or n_bootstrap <= 0:\n",
    "    print(\"-\" * 30)\n",
    "    print(\"No valid number entered.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "else:\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"You entered: {n_bootstrap}\") \n",
    "    print(\"-\" * 30)\n",
    "\n",
    "root.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532b211-c2d5-47bc-9937-d6e63afb1325",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Bootstrapping samples\n",
    "\n",
    "Run the HIV dataset simulation and estimation, based on the number of bootstrap samples you specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddffeb9-0462-4f15-930c-fe68b0e270ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create two empty lists to store the results from each bootstrap sample\n",
    "optimized_params_list = []\n",
    "\n",
    "# Calculate the initial distribution of the 4 states in the real data set for sampling the first patient.\n",
    "# For confidentiality reasons, the initial state distribution is not calculated here. Instead, I will display only the final probabilities.\n",
    "initial_dist = np.array([0.04, 0.02, 0.78, 0.16])\n",
    "\n",
    "# Calculate the number of patients from the real data set and produce the same number of simulated patients. For confidentiality reasons,\n",
    "# the number of patients from the real data set is not calculated here. Instead, I will display only the final number of total patients.\n",
    "n_simulations = 5932\n",
    "\n",
    "\n",
    "# Start the bootstrap simulation and estimation\n",
    "n_estim = 1\n",
    "while n_estim <= n_bootstrap:\n",
    "    # Set up the logging configuration for the dataset simulation\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logging.info(f'Starting simulation {n_estim}')\n",
    "    start_sim_time = time.time()\n",
    "    \n",
    "    # Create the empty data frame to store the simulated results\n",
    "    results_table = []\n",
    "    \n",
    "    for w in range(n_simulations):\n",
    "        \n",
    "        states_sample = np.arange(nstates)\n",
    "    \n",
    "        # Set the initial time and current state of the patient\n",
    "        t = 0\n",
    "        current_state = np.random.choice(a = states_sample, size = 1, p = initial_dist)[0].item()\n",
    "    \n",
    "        # Create the empty sojourn times vector to store the simulated deltaobstime\n",
    "        sojourn_times = []\n",
    "    \n",
    "        # Create the empty observation times vector to store the simulated obstime\n",
    "        observation_times = []\n",
    "        observation_times.append(t)\n",
    "    \n",
    "        # Create the empty states vector to store the simulated states\n",
    "        states = [current_state]\n",
    "        \n",
    "        patients = [w]\n",
    "    \n",
    "        C = 12\n",
    "        iteration = 1\n",
    "        \n",
    "        # Start the loop for the data simulation\n",
    "        while iteration == 1 or np.sum(sojourn_times) < C:\n",
    "        \n",
    "            patients.append(w)\n",
    "            current_state = states[iteration - 1]\n",
    "            t = observation_times[iteration - 1]\n",
    "        \n",
    "            # Calculate the probability matrix Pij based on the current state of the patient\n",
    "            P = np.zeros((nstates, nstates))\n",
    "            for i in range(nstates):\n",
    "                for j in range(nstates):\n",
    "                    if i != j:\n",
    "                        P[i,j] = hiv_smm.cpp_p(i, j, aij, bij, t, nstates)\n",
    "            \n",
    "            for i in range(nstates):\n",
    "                row_sum = P[i, :].sum()\n",
    "                if row_sum > 0:\n",
    "                    P[i, :] = P[i, :] / row_sum\n",
    "                        \n",
    "            \n",
    "            # Sample the next_state from the probability matrix\n",
    "            prob = np.nan_to_num(P[current_state, :], nan = 0.0).flatten()\n",
    "            prob = prob / np.sum(prob)\n",
    "    \n",
    "            next_state = np.random.choice(a = states_sample, size = 1, p = prob).item()\n",
    "            states.append(next_state)\n",
    "        \n",
    "            \n",
    "            # Sample the sojourn time (x) from weibull distribution\n",
    "            if iteration == 1:\n",
    "                w_shape = vij[states[0], states[1]]\n",
    "                w_scale = sij[states[0], states[1]]\n",
    "            else:\n",
    "                w_shape = vij[states[iteration - 2], states[iteration - 1]]\n",
    "                w_scale = sij[states[iteration - 2], states[iteration - 1]]\n",
    "            \n",
    "            soj_time = weibull_min.rvs(c = w_shape, scale = w_scale, size = 1)[0].item()\n",
    "            sojourn_times.append(soj_time)\n",
    "        \n",
    "            \n",
    "            # Calculate the observation_times (t)\n",
    "            obs_time = np.sum(sojourn_times[:iteration]).item()\n",
    "            observation_times.append(obs_time)\n",
    "            iteration += 1\n",
    "            \n",
    "\n",
    "        data = {\n",
    "            'PATIENT': patients,\n",
    "            'state': states,\n",
    "            'obstime': observation_times,\n",
    "            'deltaobstime': np.append(sojourn_times, np.nan)\n",
    "        }\n",
    "        \n",
    "        results_table.append(data)\n",
    "    \n",
    "    \n",
    "    results_table = pd.DataFrame(results_table)\n",
    "    \n",
    "    # Unnest the DataFrame\n",
    "    results_table = results_table.explode(['PATIENT', 'state', 'obstime', 'deltaobstime'])\n",
    "    \n",
    "    # Ensure all columns have the correct data type\n",
    "    results_table = results_table.astype({'PATIENT': int, 'state': int, 'obstime': float, 'deltaobstime': float})\n",
    "    \n",
    "    # Calculate elapsed simulation time\n",
    "    end_sim_time = time.time()\n",
    "    elapsed_sim_time = end_sim_time - start_sim_time\n",
    "    sim_minutes = int(elapsed_sim_time // 60)\n",
    "    sim_seconds = int(elapsed_sim_time % 60)\n",
    "    \n",
    "    logging.info(f'Finished! The simulation {n_estim} of the dataset lasted {sim_minutes} minutes and {sim_seconds} seconds.')\n",
    "    \n",
    "    \n",
    "    # Prepare the results_table for the simulated data parameter estimation\n",
    "    results_table['state_prev'] = results_table.groupby('PATIENT')['state'].shift(1)\n",
    "    results_table['state_prev'] = results_table['state_prev'].astype('Int64')\n",
    "    results_table['state_next'] = results_table.groupby('PATIENT')['state'].shift(-1)\n",
    "    results_table['state_next'] = results_table['state_next'].astype('Int64')\n",
    "    results_table['death'] = 0\n",
    "    results_table = results_table.reset_index(drop=True)\n",
    "    results_table['rowpos'] = results_table.index\n",
    "    patient_counts = results_table.groupby('PATIENT')['PATIENT'].transform('size')\n",
    "    results_table_c = results_table[patient_counts > 1]\n",
    "    \n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    # -------------------------------------- Start the estimation --------------------------------------\n",
    "    # --------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Set up the logging configuration for the estimation\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logging.info(f'Starting estimation of the simulated dataset {n_estim}')\n",
    "    start_est_time = time.time()\n",
    "    \n",
    "    # Define the initial parameters to start the estimation - They should be the estimated parameters from the real data set.\n",
    "    # Load the RData files again\n",
    "    aij_r = pyreadr.read_r(aij_path)\n",
    "    bij_r = pyreadr.read_r(bij_path)\n",
    "    vij_r = pyreadr.read_r(vij_path)\n",
    "    sij_r = pyreadr.read_r(sij_path)\n",
    "    \n",
    "    aij_s = jnp.array(aij_r['aij'].to_numpy().T.flatten())\n",
    "    bij_s = jnp.array(bij_r['bij'].to_numpy().T.flatten())\n",
    "    vij_s = jnp.array(vij_r['vij'].to_numpy().T.flatten())\n",
    "    sij_s = jnp.array(sij_r['sij'].to_numpy().T.flatten())\n",
    "    \n",
    "    # Put the parameters into one vector for the optimization\n",
    "    params_s = np.concatenate((vij_s, sij_s, aij_s, bij_s))\n",
    "    \n",
    "    # Enable double-precission floating-point to maximize speed\n",
    "    jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "    # Start the estimation of the parameters. Optimize the loglikelihood function\n",
    "    result = minimize(\n",
    "        fun=load_functions.fun_wrapper,\n",
    "        x0=params_s,\n",
    "        jac=load_functions.jac_wrapper,\n",
    "        args=(results_table_c, nstates, parscale),\n",
    "        method='BFGS',\n",
    "        options={'maxiter': 20000}\n",
    "    )\n",
    "\n",
    "            \n",
    "    # Get the optimized parameters:\n",
    "    optimized_params = result.x\n",
    "    print(f'Sample {n_estim} estimated parameters: \\n, {optimized_params} \\n')\n",
    "\n",
    "    # The maximum log-likelihood value is the negative of the final fun value:\n",
    "    max_log_likelihood = -result.fun\n",
    "    print(f'Sample {n_estim} loglikelihood value: \\n, {max_log_likelihood}, \\n')\n",
    "\n",
    "    # Append the results to the lists\n",
    "    optimized_params_list.append(optimized_params)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    end_est_time = time.time()\n",
    "    elapsed_est_time = end_est_time - start_est_time\n",
    "    est_minutes = int(elapsed_est_time // 60)\n",
    "    est_seconds = int(elapsed_est_time % 60)\n",
    "\n",
    "    \n",
    "    logging.info(f'Finished! The estimation of the simulated dataset {n_estim} lasted {est_minutes} minutes and {est_seconds} seconds.')\n",
    "\n",
    "    # Start the next iteration\n",
    "    n_estim += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d6e66-868d-4921-ab07-fe41dfc1514f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Arrange the estimated parameters\n",
    "\n",
    "Build the $v_{ij}$, $s_{ij}$, $a_{ij}$, $b_{ij}$ parameter lists from the simulated data. These will be used to calculate some descriptive statistics for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebe067-2aec-4bc5-8fad-ba62367f7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vij_list = []\n",
    "sij_list = []\n",
    "aij_list = []\n",
    "bij_list = []\n",
    "\n",
    "# Calculate the expected length based on nstates\n",
    "expected_length = 4 * nstates**2\n",
    "\n",
    "# Iterate through each array in the input list\n",
    "for params in optimized_params_list:\n",
    "\n",
    "    # Calculate the size of each parameter section\n",
    "    part_size = nstates**2\n",
    "\n",
    "    # vij\n",
    "    vij = np.abs(params[0:part_size].reshape(nstates, nstates)) / parscale\n",
    "\n",
    "    # sij\n",
    "    sij = np.abs(params[part_size:2*part_size].reshape(nstates, nstates)) / parscale\n",
    "\n",
    "    # aij\n",
    "    aij = params[2*part_size:3*part_size].reshape(nstates, nstates)\n",
    "\n",
    "    # bij\n",
    "    bij = params[3*part_size:4*part_size].reshape(nstates, nstates)\n",
    "    bij[nstates - 1, nstates - 2] = 1 - np.sum(bij[nstates - 1, 0:nstates - 2])\n",
    "    bij[:, nstates - 1] = 1 - np.sum(bij[:, 0:nstates - 1], axis=1)\n",
    "\n",
    "    # Append the resulting matrices to their respective lists\n",
    "    vij_list.append(vij)\n",
    "    sij_list.append(sij)\n",
    "    aij_list.append(aij)\n",
    "    bij_list.append(bij)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d5278-fef5-4989-9a20-af2f53591afa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Descriptive statistics\n",
    "\n",
    "Calculate the descriptive statistics for each parameter.\n",
    "\n",
    "**Note:** *NaN values in the t-value or p-value indicate the diagonal elements of the matrices, which are not relevant for our calculations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181637cb-9bbd-4e83-b138-d8cd8401b038",
   "metadata": {},
   "source": [
    "#### Statistics for $v_{ij}$ parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319d8a7-5890-4959-84e1-6efca7fd0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 100)\n",
    "print(load_functions.analyze_parameters(vij_list))\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28314dde-ce2a-4d1d-a31a-a00b75ee87b1",
   "metadata": {},
   "source": [
    "#### Statistics for $s_{ij}$ parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6363625-fe20-4e5b-851b-2314a47a0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 100)\n",
    "print(load_functions.analyze_parameters(sij_list))\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac476fbe-3a8f-44b4-b012-240567bf38c0",
   "metadata": {},
   "source": [
    "#### Statistics for $a_{ij}$ parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ffa8e-4eb7-4b9d-ba6b-8b03bbc75f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 100)\n",
    "print(load_functions.analyze_parameters(aij_list))\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e08438-95e7-4c4b-ae37-c9f00039a08c",
   "metadata": {},
   "source": [
    "#### Statistics for $b_{ij}$ parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b7f3f-9ba8-4ef6-bf86-8ed00187f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 100)\n",
    "print(load_functions.analyze_parameters(bij_list))\n",
    "print(\"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
